{"cells":[{"cell_type":"code","source":["import glob\n","import pandas as pd\n","\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import col, when, lit, date_format, to_date, current_timestamp\n","from delta.tables import *\n","\n","class SACrimeRecordWrangler:\n","\n","    @staticmethod\n","    def create_init_schema():\n","        return StructType([\n","            StructField('ReportedDate', StringType()),\n","            StructField('Suburb', StringType()),\n","            StructField('Postcode', StringType()),\n","            StructField('LevelOneDesc', StringType()),\n","            StructField('LevelTwoDesc', StringType()),\n","            StructField('LevelThreeDesc', StringType()),\n","            StructField('Count', IntegerType())\n","        ])\n","\n","\n","    @staticmethod\n","    def load_excel_files(spark, url, schema):\n","        excel_files = glob.glob(f'/lakehouse/default/{url}/*.xlsx')\n","        df = pd.concat((pd.read_excel(f) for f in excel_files))\n","        return spark.createDataFrame(df, schema)\n","\n","\n","    @staticmethod\n","    def load_csv_files(spark, url, schema):\n","        return spark.read \\\n","            .format('csv') \\\n","            .option('header', 'true') \\\n","            .schema(schema) \\\n","            .load(f'{url}/*.csv')\n","\n","\n","    @staticmethod\n","    def remove_all_na(df):\n","        return df.na.drop(how='all')\n","\n","\n","    @staticmethod\n","    def reported_date_str_to_date(df):\n","        return df.withColumn('ReportedDate', to_date(col('ReportedDate'), 'd/M/y'))\n","\n","\n","    @staticmethod\n","    def postcode_str_to_short(df):\n","        return df.withColumn('Postcode', col('Postcode').cast(ShortType()))\n","\n","\n","    @staticmethod\n","    def suburb_fill_null_empty(df):\n","        return df.withColumn('Suburb', when((col('Suburb').isNull() | (col('Suburb') == '')), lit('N/A')).otherwise(col('Suburb')))\n","\n","\n","    @staticmethod\n","    def postcode_fill_null(df):\n","        return df.na.fill(value=0, subset=['Postcode'])\n","\n","\n","    @staticmethod\n","    def cleanse_df(df):\n","        tmp_df = SACrimeRecordWrangler.remove_all_na(df)\n","        tmp_df = SACrimeRecordWrangler.reported_date_str_to_date(tmp_df)\n","        tmp_df = SACrimeRecordWrangler.postcode_str_to_short(tmp_df)\n","        tmp_df = SACrimeRecordWrangler.suburb_fill_null_empty(tmp_df)\n","        return SACrimeRecordWrangler.postcode_fill_null(tmp_df)\n","\n","\n","    @staticmethod\n","    def create_crime_records_silver_table(spark_session, table_name):\n","        DeltaTable.createIfNotExists(spark_session) \\\n","            .tableName(table_name) \\\n","            .addColumn('ReportedDate', DateType()) \\\n","            .addColumn('Suburb', StringType()) \\\n","            .addColumn('Postcode', ShortType()) \\\n","            .addColumn('LevelOneDesc', StringType()) \\\n","            .addColumn('LevelTwoDesc', StringType()) \\\n","            .addColumn('LevelThreeDesc', StringType()) \\\n","            .addColumn('Count', IntegerType()) \\\n","            .addColumn('UpdatedTS', TimestampType()) \\\n","            .execute()\n","\n","\n","    @staticmethod\n","    def upsert_delta_table(delta_table, df):\n","\n","        df_updates = df\n","\n","        match_condition = (\n","            'silver.ReportedDate = updates.ReportedDate and '\n","            'silver.Suburb = updates.Suburb and '\n","            'silver.Postcode = updates.Postcode and '\n","            'silver.LevelOneDesc = updates.LevelOneDesc and '\n","            'silver.LevelTwoDesc = updates.LevelTwoDesc and '\n","            'silver.LevelThreeDesc = updates.LevelThreeDesc'\n","        )\n","        \n","        delta_table.alias('silver') \\\n","            .merge(\n","                df_updates.alias('updates'),\n","                match_condition\n","            ) \\\n","            .whenMatchedUpdate(\n","                condition='silver.Count != updates.Count',\n","                set=\n","                {\n","                    'Count': 'updates.Count',\n","                    'UpdatedTS': current_timestamp()\n","                }\n","            ) \\\n","            .whenNotMatchedInsert(values=\n","                {\n","                    'ReportedDate': 'updates.ReportedDate',\n","                    'Suburb': 'updates.Suburb',\n","                    'Postcode': 'updates.Postcode',\n","                    'LevelOneDesc': 'updates.LevelOneDesc',\n","                    'LevelTwoDesc': 'updates.LevelTwoDesc',\n","                    'LevelThreeDesc': 'updates.LevelThreeDesc',\n","                    'Count': 'updates.Count',\n","                    'UpdatedTS': current_timestamp()\n","                }\n","            ) \\\n","            .execute()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea7f4dfc-6b20-4dae-b130-6aa6bae5aea0"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}