{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import monotonically_increasing_id, row_number, col, when, coalesce, max, lit\n","from pyspark.sql.types import *\n","from pyspark.sql.window import Window\n","from delta.tables import DeltaTable\n","\n","class DimDescWrangler:\n","\n","    @staticmethod\n","    def extract_silver_df(silver_df):\n","        return silver_df.dropDuplicates(['LevelOneDesc', 'LevelTwoDesc', 'LevelThreeDesc']).select(\n","            col('LevelOneDesc'),\n","            col('LevelTwoDesc'),\n","            col('LevelThreeDesc')\n","        )\n","\n","\n","    @staticmethod\n","    def create_delta_table(spark_session, table_name):\n","        DeltaTable.createIfNotExists(spark_session) \\\n","            .tableName(table_name) \\\n","            .addColumn('LevelOneDesc', StringType()) \\\n","            .addColumn('LevelTwoDesc',  StringType()) \\\n","            .addColumn('LevelThreeDesc', StringType()) \\\n","            .addColumn('DescID', IntegerType()) \\\n","            .execute()\n","\n","    \n","    @staticmethod\n","    def setup_id(spark_session, table_name, upsert_df):\n","        table_df = spark.read.table(table_name)\n","    \n","        max_desc_id = table_df.select(coalesce(max(col('DescID')), lit(0)).alias('MAXDescID')).first()[0]\n","\n","        upsert_df = upsert_df.join(\n","            table_df, \n","            (upsert_df.LevelOneDesc == table_df.LevelOneDesc) & (upsert_df.LevelTwoDesc == table_df.LevelTwoDesc) & (upsert_df.LevelThreeDesc == table_df.LevelThreeDesc),\n","            'left_anti'\n","        )\n","\n","        upsert_df = upsert_df.withColumn('DescID', monotonically_increasing_id())\n","        window = Window.orderBy('DescID')\n","        return upsert_df.withColumn('DescID', row_number().over(window) + max_desc_id)\n","\n","\n","    @staticmethod\n","    def upsert_delta_table(delta_table, df):\n","\n","        df_updates = df\n","\n","        delta_table.alias('existing') \\\n","            .merge(\n","                df_updates.alias('updates'),\n","                'existing.LevelOneDesc = updates.LevelOneDesc and existing.LevelTwoDesc = updates.LevelTwoDesc and existing.LevelThreeDesc = updates.LevelThreeDesc'\n","            ) \\\n","            .whenMatchedUpdate(set=\n","                {  \n","                }\n","            ) \\\n","            .whenNotMatchedInsert(values=\n","                {\n","                    'LevelOneDesc': 'updates.LevelOneDesc',\n","                    'LevelTwoDesc': 'updates.LevelTwoDesc',\n","                    'LevelThreeDesc': 'updates.LevelThreeDesc',\n","                    'DescID': 'updates.DescID'\n","                }\n","            ) \\\n","            .execute()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b1ab1c26-c6af-45f0-90f8-5b98b3a209bc"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}