{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col\n","from pyspark.sql.types import *\n","from delta.tables import DeltaTable\n","\n","class FactRecordWrangler:\n","\n","    @staticmethod\n","    def extract_silver_df(silver_df, dim_desc_table):\n","\n","        return silver_df.alias('df1').join(\n","            dim_desc_table.alias('df2'),\n","            (silver_df.LevelOneDesc == dim_desc_table.LevelOneDesc) & (silver_df.LevelTwoDesc == dim_desc_table.LevelTwoDesc) & (silver_df.LevelThreeDesc == dim_desc_table.LevelThreeDesc),\n","            'left'\n","        ) \\\n","        .select(\n","            col('df1.ReportedDate'), \\\n","            col('df1.Suburb'), \\\n","            col('df1.Postcode'), \\\n","            col('df2.DescID'), \\\n","            col('df1.Count'), \\\n","        ) \\\n","        .orderBy(col('df1.ReportedDate'), col('df2.DescID'))\n","\n","    @staticmethod\n","    def create_delta_table(spark_session, table_name):\n","        DeltaTable.createIfNotExists(spark_session) \\\n","            .tableName(table_name) \\\n","            .addColumn('ReportedDate', DateType()) \\\n","            .addColumn('Suburb', StringType()) \\\n","            .addColumn('Postcode', ShortType()) \\\n","            .addColumn('DescID', IntegerType()) \\\n","            .addColumn('Count', IntegerType()) \\\n","            .execute()\n","\n","    @staticmethod\n","    def upsert_delta_table(delta_table, df):\n","\n","        df_updates = df\n","    \n","        delta_table.alias('gold') \\\n","            .merge(\n","                df_updates.alias('updates'),\n","                'gold.ReportedDate = updates.ReportedDate and gold.Suburb = updates.Suburb and gold.Postcode = updates.Postcode and gold.DescID = updates.DescID'\n","            ) \\\n","            .whenMatchedUpdate(\n","                condition='gold.Count != updates.Count',\n","                set=\n","                {\n","                    'gold.Count': 'updates.Count'\n","                }\n","            ) \\\n","            .whenNotMatchedInsert(values=\n","                {\n","                    'ReportedDate': 'updates.ReportedDate',\n","                    'Suburb': 'updates.Suburb',\n","                    'Postcode': 'updates.Postcode',\n","                    'DescID': 'updates.DescID',\n","                    'Count': 'updates.Count'\n","                }\n","            ) \\\n","            .execute()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e87747df-688d-422e-81d2-5d3cdaee18ff"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}